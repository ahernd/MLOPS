


!pip uninstall joblib scikit-learn


%pip install joblib scikit-learn





# Import necessary libraries
import pandas as pd
from sklearn.datasets import fetch_california_housing

# Step 1: Load the California housing dataset from sklearn
housing = fetch_california_housing(as_frame=True)  # Calling the function
df = housing.frame  # The dataframe for California housing data
print("\nData loaded successfully.")

# Step 2: Check the shape of the dataset (rows and columns)
rows, cols = df.shape
print(f"\nThe dataset contains {rows} rows and {cols} columns.\n")

# Step 3: Print the column names
print(f"The dataset has {cols} columns. The column names are:\n{list(df.columns)}\n")

# Step 4: Separate numeric and categorical variables
# The California housing dataset is primarily numeric
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
# Since there are no categorical variables in this dataset, we create an empty list for categorical variables
categorical_columns = df.select_dtypes(include=['object']).columns

print(f"Numeric Variables ({len(numeric_columns)} columns):\n{list(numeric_columns)}\n")
print(f"Categorical Variables ({len(categorical_columns)} columns):\n{list(categorical_columns)}\n")

# Step 5: Get an overview of the dataset using df.info()
print("\nDataset Information:\n")
df.info()

# Step 6: Checking for null/missing values
missing_values = df.isnull().sum()
missing_values = missing_values[missing_values > 0]  # Only display columns with missing values
print("\nMissing Values Summary (Columns with missing values):\n")
print(missing_values if not missing_values.empty else "No missing values found in the dataset.")

# Optional: Display the first few rows of the dataset to visually inspect it
print("\nFirst 5 rows of the dataset:")
print(df.head())

# Get summary statistics for numerical columns
print("\nDescriptive statistics for the dataset:\n")
print(df.describe())




# Generate the Pandas Profiling Report
print("\nGenerating Pandas Profiling Report...\n")
profile = ProfileReport(df, title="California Housing Data Profiling Report", explorative=True)

# Export the report to an HTML file
profile.to_file("california_housing_profile_report.html")

print("\nPandas Profiling report generated and saved as 'california_housing_profile_report.html'.")

#Print the report on the Jupyter notebook
print("\nPadas Profiling Report...\n")
profile


import sys
print("Python version:", sys.version)






# Add feature interactions (example)
df['Rooms_per_House'] = df['AveRooms'] / df['HouseAge']






from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)












from sklearn.model_selection import train_test_split

X = df_scaled.drop('Price', axis=1)
y = df_scaled['Price']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)






from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV

# Initialize model
rf = RandomForestRegressor()

# Hyperparameter tuning
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)
grid_search.fit(X_train, y_train)

print(f"Best Parameters: {grid_search.best_params_}")






best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(y_test, y_pred)
print(f"RMSE: {mse ** 0.5}")











import shap

# Initialize SHAP explainer
explainer = shap.TreeExplainer(best_rf)
shap_values = explainer.shap_values(X_test)

# Plot SHAP values
shap.summary_plot(shap_values, X_test)






import lime
import lime.lime_tabular

explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values, feature_names=X.columns, class_names=['Price'])
exp = explainer.explain_instance(X_test.iloc[0], best_rf.predict)
exp.show_in_notebook(show_table=True)











import joblib

# Save model
joblib.dump(best_rf, 'best_rf_model.pkl')






from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# Load the model
model = joblib.load('best_rf_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
    data = request.json
    prediction = model.predict([data['features']])
    return jsonify({'prediction': prediction[0]})

if __name__ == '__main__':
    app.run(debug=True)







